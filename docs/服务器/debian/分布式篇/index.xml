<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>分布式篇 on </title>
    <link>https://example.org/docs/%E6%9C%8D%E5%8A%A1%E5%99%A8/debian/%E5%88%86%E5%B8%83%E5%BC%8F%E7%AF%87/</link>
    <description>Recent content in 分布式篇 on </description>
    <generator>Hugo -- gohugo.io</generator>
    <language>zh</language>
    <atom:link href="https://example.org/docs/%E6%9C%8D%E5%8A%A1%E5%99%A8/debian/%E5%88%86%E5%B8%83%E5%BC%8F%E7%AF%87/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Hadoop 3.4.1 单机版</title>
      <link>https://example.org/docs/%E6%9C%8D%E5%8A%A1%E5%99%A8/debian/%E5%88%86%E5%B8%83%E5%BC%8F%E7%AF%87/hadoop-singlecluster/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://example.org/docs/%E6%9C%8D%E5%8A%A1%E5%99%A8/debian/%E5%88%86%E5%B8%83%E5%BC%8F%E7%AF%87/hadoop-singlecluster/</guid>
      <description>Hadoop 3.4.1 单机版 # https://hadoop.apache.org/docs/current/hadoop-project-dist/hadoop-common/SingleCluster.html 目前JDK11能正常使用,JDK17网页无法使用上传下载 root操作 # sed -i &amp;#34;s|bookworm-updates|bookworm-updates bullseye|g&amp;#34; /etc/apt/sources.list.d/debian.sources apt update &amp;amp;&amp;amp; apt install -y openjdk-11-jdk sudo apt install openjdk-11-jdk-headless -y sed -i &amp;#39;$aPATH=\$PATH:/usr/games:/opt/hadoop-3.4.1/bin:/opt/hadoop-3.4.1/sbin&amp;#39; /etc/profile sed -i &amp;#39;$aexport JAVA_HOME=/usr/lib/jvm/java-11-openjdk-amd64/&amp;#39; /etc/profile sudo useradd -ms/bin/bash -k/etc/skel/ hadoop sudo usermod -aG sudo hadoop echo hadoop:. | chpasswd cd /opt wget https://dlcdn.apache.org/hadoop/common/hadoop-3.4.1/hadoop-3.4.1.tar.gz tar -zxvf hadoop-3.4.1.tar.gz sed -i &amp;#34;1 i JAVA_HOME=/usr/lib/jvm/java-11-openjdk-amd64&amp;#34; /opt/hadoop-3.4.1/etc/hadoop/hadoop-env.sh mv /opt/hadoop-3.4.1.tar.gz /opt/hadoop-3.4.1 find /opt/hadoop-3.4.1/ -name *cmd -delete mkdir /opt/hadoop-3.</description>
    </item>
    <item>
      <title>Hadoop 3.4.1 集群版</title>
      <link>https://example.org/docs/%E6%9C%8D%E5%8A%A1%E5%99%A8/debian/%E5%88%86%E5%B8%83%E5%BC%8F%E7%AF%87/hadoop-clustersetup/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://example.org/docs/%E6%9C%8D%E5%8A%A1%E5%99%A8/debian/%E5%88%86%E5%B8%83%E5%BC%8F%E7%AF%87/hadoop-clustersetup/</guid>
      <description>Hadoop 3.4.1 集群版之HDFS # https://blog.csdn.net/tang5615/article/details/120382513&#xA;root操作(建议docker打包) # sed -i &amp;#34;s|bookworm-updates|bookworm-updates bullseye|g&amp;#34; /etc/apt/sources.list.d/debian.sources apt update &amp;amp;&amp;amp; apt install -y openjdk-11-jdk sudo apt install openjdk-11-jdk-headless -y sed -i &amp;#39;$aPATH=\$PATH:/usr/games:/opt/hadoop-3.4.1/bin:/opt/hadoop-3.4.1/sbin&amp;#39; /etc/profile sed -i &amp;#39;$aexport JAVA_HOME=/usr/lib/jvm/java-11-openjdk-amd64/&amp;#39; /etc/profile sudo useradd -ms/bin/bash -k/etc/skel/ hadoop sudo usermod -aG sudo hadoop echo hadoop:. | chpasswd cd /opt wget https://dlcdn.apache.org/hadoop/common/hadoop-3.4.1/hadoop-3.4.1.tar.gz tar -zxvf hadoop-3.4.1.tar.gz sed -i &amp;#34;1 i JAVA_HOME=/usr/lib/jvm/java-11-openjdk-amd64&amp;#34; /opt/hadoop-3.4.1/etc/hadoop/hadoop-env.sh mv /opt/hadoop-3.4.1.tar.gz /opt/hadoop-3.4.1 find /opt/hadoop-3.4.1/ -name *cmd -delete mkdir /opt/hadoop-3.</description>
    </item>
    <item>
      <title>Hadoop 3.4.1 运维篇</title>
      <link>https://example.org/docs/%E6%9C%8D%E5%8A%A1%E5%99%A8/debian/%E5%88%86%E5%B8%83%E5%BC%8F%E7%AF%87/hadoop-%E8%BF%90%E7%BB%B4%E7%AF%87/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://example.org/docs/%E6%9C%8D%E5%8A%A1%E5%99%A8/debian/%E5%88%86%E5%B8%83%E5%BC%8F%E7%AF%87/hadoop-%E8%BF%90%E7%BB%B4%E7%AF%87/</guid>
      <description>hadoop 3.4.1 运维篇 # 块损坏与丢失 # 损坏或丢失会进入安全模式 hdfs dfsadmin -safemode get&#x9;#功能描述:查看安全模式状态 hdfs dfsadmin -safemode enter #进入安全模式状态 hdfs dfsadmin -safemode leave #离开安全模式状态 hdfs dfsadmin -safemode wait #等待安全模式状态 hadoop fsck #检查时候存在丢失或者损坏 hdfs fsck / -delete # 删除问题文件,在获得问题的解决办法前不建议使用 测试数据 # 写入100个128M的数据 for x in {1..100};do dd if=/dev/random of=$RANDOM bs=1M count=128;done hadoop fs -put * /4m 查找文件 # find /opt/ -mmin -30 find /opt/ -exec touch -ht 197001010000.00 {} \; rm /opt/hadoop-3.4.1/data/* /opt/hadoop-3.</description>
    </item>
  </channel>
</rss>
